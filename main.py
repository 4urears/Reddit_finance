# -*- coding: utf-8 -*-
"""Reddit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mf0xmM6ypCHOAL6FHub5C43-J09w1sRL
"""

!pip install praw
!pip install llama-index llama-hub
!pip install llama-index-embeddings-huggingface
!pip install llama-index-embeddings-instructor
!`pip install accelerate
!pip install git+https://github.com/huggingface/accelerate
#docker pull langchain/langchain
#Fast API token:

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --quiet  langchain langsmith langchainhub --quiet

# Import Colab Secrets userdata module
from google.colab import userdata

import os
from uuid import uuid4

unique_id = uuid4().hex[0:8]
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = f"Reddit_finance" #f"AtomCamp - {unique_id}"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_API_KEY"] =  userdata.get('Langsmith')  # Update to your API key

#langsmith api key: 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
from langchain import hub
from langchain.agents import AgentExecutor

from langsmith import Client

client = Client()

!pip install fastapi nest-asyncio pyngrok uvicorn

!ngrok config add-authtoken userdata.get('fast')

from fastapi import FastAPI
import nest_asyncio
from pyngrok import ngrok
import uvicorn
import requests
import torch

import praw
from llama_index.core import SimpleDirectoryReader
from textblob import TextBlob
import datetime
from llama_index.core.llama_pack import download_llama_pack
ZephyrQueryEnginePack = download_llama_pack(
    "ZephyrQueryEnginePack", "./zephyr_pack"
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# Initialize Reddit instance
reddit = praw.Reddit(client_id=userdata.get('Reddit_client'),
                     client_secret=userdata.get('Reddi_secret'),
                     user_agent=True )

# Choose the subreddit you want to scrape
subreddit = reddit.subreddit('stocks')

# Define the time range (last 6 months)
start_date = datetime.datetime.utcnow() - datetime.timedelta(days=180)
start_date_timestamp = int(start_date.timestamp())

# Open a text file for writing
with open('reddit_data.txt', 'w', encoding='utf-8') as file:
    # Collect posts from the subreddit for the last 6 months
    for submission in subreddit.new(limit=None):
        if submission.created_utc > start_date_timestamp:
            # Write submission title and body to the text file
            file.write("Title: " + submission.title + "\n")
            file.write("Body: " + submission.selftext + "\n")
            file.write("\n")

# Print a message indicating that the data has been saved
#print("Reddit data has been saved to reddit_data.txt")

reader = SimpleDirectoryReader(
    input_files=["/content/reddit_data.txt"]
)

documents=reader.load_data()

zephyr_pack = ZephyrQueryEnginePack(documents)

response = zephyr_pack.run(
    "most talked about stocks?", similarity_top_k=2
)
print(str(response))

app = FastAPI()

@app.get('/')
async def index():
 return {'message':'Hello,World'}

@app.post("/prompt")
async def prompt(prompt_data: dict):

    user_prompt = prompt_data["prompt"]
    response = await zephyr_pack.run(user_prompt, similarity_top_k=2)
    return {"response": str(response)}

@app.get('/t')
async def index(input: str):
    repeated_input = input * 10
    return {'message': repeated_input}
@app.get('/test/{user_input}')
async def test(user_input: str):
    response =zephyr_pack.run(user_input, similarity_top_k=2)
    return {"response": str(response)}

ngrok_tunnel = ngrok.connect(8000)
print('Public URL:', ngrok_tunnel.public_url)
nest_asyncio.apply()
uvicorn.run(app, port=8000)

